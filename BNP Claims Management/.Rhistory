g + geom_point(aes(x=temperature)) + geom_abline(intercept=-140, slope=2.3, mapping=aes(color='red', size=2)) + annotate('text', x=70, y=75, color='blue', size=10, family='Arial') + theme(axis.title=element_text(size=16))
g + geom_point(aes(x=temperature)) + geom_abline(intercept=-140, slope=2.3, mapping=aes(color='red', size=2)) + annotate('text', x=70, y=75, label='\u03B8\u2080 = -140, \u03B8\u2081 = 2.3', color='blue', size=10, family='Arial') + theme(axis.title=element_text(size=16))
g + geom_point(aes(x=temperature)) + geom_abline(intercept=-140, slope=2.3, mapping=aes(color='red', size=2)) + annotate('text', x=70, y=75, label='\u03B8\u2080 = -140, \u03B8\u2081 = 2.3', color='blue', size=10, family='Arial') + theme(axis.title=element_text(size=16), axis.text=element_text(size=10))
g + geom_point(aes(x=temperature)) + geom_abline(intercept=-140, slope=2.3, mapping=aes(color='red', size=2)) + annotate('text', x=70, y=75, label='\u03B8\u2080 = -140, \u03B8\u2081 = 2.3', color='blue', size=10, family='Arial') + theme(axis.title=element_text(size=16), axis.text=element_text(size=12))
g + geom_point(aes(x=temperature)) + geom_abline(intercept=-140, slope=2.3, mapping=aes(color='red', size=2)) + annotate('text', x=70, y=75, label='\u03B8\u2080 = -140, \u03B8\u2081 = 2.3', color='blue', size=10, family='Arial') + theme(axis.title=element_text(size=16), axis.text=element_text(size=13))
g + geom_point(aes(x=temperature)) + geom_abline(intercept=-140, slope=2.3, mapping=aes(color='red', size=2)) + annotate('text', x=70, y=75, label='\u03B8\u2080 = -140, \u03B8\u2081 = 2.3', color='blue', size=10, family='Arial') + theme(axis.title=element_text(size=16), axis.text=element_text(size=14))
g + geom_point(aes(x=temperature)) + geom_abline(intercept=-140, slope=2.3, mapping=aes(color='red', size=2)) + annotate('text', x=70, y=75, label='\u03B8\u2080 = -140, \u03B8\u2081 = 2.3', color='blue', size=10, family='Arial') + theme(axis.title=element_text(size=18), axis.text=element_text(size=14))
g + geom_point(aes(x=temperature)) + geom_abline(intercept=-140, slope=2.3, mapping=aes(color='red', size=2)) + annotate('text', x=70, y=75, label='\u03B8\u2080 = -140, \u03B8\u2081 = 2.3', color='blue', size=14, family='Arial') + theme(axis.title=element_text(size=16), axis.text=element_text(size=14))
g + geom_point(aes(x=temperature)) + geom_abline(intercept=-140, slope=2.3, mapping=aes(color='red', size=2)) + theme(axis.title=element_text(size=16), axis.text=element_text(size=14))
g <- ggplot(data=ozone.test, aes(y=ozone))
g + geom_point(aes(x=temperature)) + geom_abline(intercept=-140, slope=2.3, mapping=aes(color='red', size=2)) + theme(axis.title=element_text(size=16), axis.text=element_text(size=14))
ozone.lm.residuals <- ozone.test$ozone - ozone.lm.predictions
plot(ozone.test$ozone, ozone.lm.predictions)
plot(ozone.test$ozone, ozone.lm.predictions, xlab='True Value', ylab='Predicted Value')
plot(ozone.test$ozone, ozone.lm.predictions, xlab='True Value', ylab='Predicted Value', main='Ozone Test True vs Predicted)
plot(ozone.test$ozone, ozone.lm.predictions, xlab='True Value', ylab='Predicted Value', main='Ozone Test True vs Predicted')
plot(ozone.test$ozone, ozone.lm.residuals, xlab='True Value', ylab='Residual', main='Ozone Test True Value vs Residuals')
plot(ozone.test$ozone, ozone.lm.residuals, xlab='True Value', ylab='Residuals', main='Ozone Test True Value vs Residuals')
ozone.lm.rmse <- sqrt(mean(ozone.lm.residuals^2))
print(ozone.lm.rmse)
ozone.lm.mae <- mean(abs(ozone.lm.residuals))
print(ozone.lm.mae)
head(ozone.test)
head
head(ozone.lm.predictions)
head(abs(ozone.lm.predictions-ozone.test$ozone))
mean(head(abs(ozone.lm.predictions-ozone.test$ozone)))
sum(head(abs(ozone.lm.predictions-ozone.test$ozone)))
mean(head(abs(ozone.lm.predictions-ozone.test$ozone))^2)
head(abs(ozone.lm.predictions-ozone.test$ozone))^2
sum(head(abs(ozone.lm.predictions-ozone.test$ozone))^2)
sqrt(mean((head(abs(ozone.lm.predictions-ozone.test$ozone))^2))
)
ozone.lm.r2 <- rSquared(ozone.lm.predictions, ozone.lm.residuals)
library(miscTools)
ozone.lm.r2 <- rSquared(ozone.lm.predictions, ozone.lm.residuals)
print(ozone.lm.r2)
install.packages('RCurl')
library(RCurl)
output.raw <- getURL("https://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.data", .opts = list(ssl.verifypeer = FALSE))
data.raw <- read.table(text = output.raw, header = FALSE)
setwd("~/")
write.table(data.raw, "AutoMPG.txt", sep="\t")
auto.data <- read.table("autoMPG.txt", header = TRUE)
auto.names <- c("mpg", "cyl", "disp", "horsepow", "weight", "accel", "year", "country", "name")
colnames(auto.data) <- auto.names
auto.data[auto.data = "?",] <- NA
auto.data[auto.data == "?",] <- NA
auto.data[auto.data == "?" <- NA]
auto.data <- is.na(auto.data) == NA
View(auto.data)
View(auto.data)
auto.data <- read.table("autoMPG.txt", header = TRUE)
auto.names <- c("mpg", "cyl", "disp", "horsepow", "weight", "accel", "year", "country", "name")
auto.data[auto.data == "?"] <- NA
colnames(auto.data) <- auto.names
auto.data <- read.table("autoMPG.txt", header = TRUE)
colnames(auto.data) <- auto.names
auto.data[auto.data == "?", 'horsepow'] <- NA
auto.data[auto.data$horsepow == "?", 'horsepow'] <- NA
nrow(subset(auto.subset, mpg > 20))
auto.subset <- auto.clean[c(1,2)]
auto.clean <- na.omit(auto.data)
auto.subset <- auto.clean[,c(1:2)]
nrow(subset(auto.subset, mpg > 20))
length(auto.subset$mpg[auto.subset$mpg > 20])
nrow(auto.subset$mpg[mpg > 20])
length(auto.subset[mpg > 20])
True
TREUE
TRUE
libray(plyr)
library(plyr)
?plyr
data()
head(mtcars)
?head
tail(mtcars)
tail(mtcars, n=-4)
dim(mtcars)
colnames(mtcars)
colnames(mtcars)[2] <- "cylinders"
colnames(mtcars)
?mtcars
?boxplot
?write.csv
?read.csv
mtcars[['gear']] <- as.factor(mtcars[['gear']])
summary(mtcars)
mtcars.4 <- mtcars[mtcars$cyl==4,]
head(mycars.4)
head(mtcars.4)
mtcars.4 <- mtcars[mtcars$cyl==4]
mtcars.4 <- mtcars[mtcars$cyl==4,'cyl']
mtcars.4 <- mtcars[mtcars$cyl==4,'cyl']
mtcars.4 <- mtcars[mtcars$cyl==4,]
?write.csv
?read.csv
?party
library(party)
?party
?ctree
library(rpart)
?rpart
?rpart
library(rpart)
## DATA EXPLORATION
## load the iris data in R
data(iris)
## explore the data set
str(iris)
dim(iris)
summary(iris)
## BUILD MODEL
## randomly choose 70% of the data set as training data
set.seed(27)
iris.train.indices <- sample(1:nrow(iris), 0.7*nrow(iris), replace=F)
iris.train <- iris[iris.train.indices,]
dim(iris.train)
## select the 30% left as the testing data
iris.test <- iris[-iris.train.indices,]
dim(iris.test)
## You could also do this
#iris.test.indices <- setdiff(1:nrow(iris),random.rows.train)
#iris.test <- iris[random.rows.test,]
## Setting control parameters for rpart
## Check ?rpart.control for what the parameters do
iris.dt.parameters <- rpart.control(minsplit=10, minbucket=3, cp=0.005, maxdepth=30)
## Fit decision model to training set
## Use parameters from above and Gini index for splitting
iris.dt.model <- rpart(Species ~ ., data = iris.train,
control=iris.dt.parameters, parms=list(split="gini"))
## VISUALIZE THE MODEL
## plot the tree structure
plot(iris.dt.model)
title(main = "Decision Tree Model of Iris Data")
text(iris.dt.model, use.n = TRUE)
## print the tree structure
summary(iris.dt.model)
iris.dt.predictions <- predict(iris.dt.model, iris.test, type = "class")
## Extract the test data species to build the confusion matrix
iris.dt.confusion <- table(iris.dt.predictions, iris.test$Species)
print(iris.dt.confusion)
iris.dt.accuracy <- sum(diag(iris.dt.confusion)) / sum(iris.dt.confusion)
print(iris.dt.accuracy)
iris.dt.parameters <- rpart.control(minsplit=20, minbucket=7, cp=0.01, maxdepth=30)
## Fit decision model to training set
## Use parameters from above and Gini index for splitting
iris.dt.model <- rpart(Species ~ ., data = iris.train,
control=iris.dt.parameters, parms=list(split="gini"))
## VISUALIZE THE MODEL
## plot the tree structure
plot(iris.dt.model)
title(main = "Decision Tree Model of Iris Data")
text(iris.dt.model, use.n = TRUE)
## print the tree structure
summary(iris.dt.model)
## MODEL EVALUATION
## make prediction using decision model
iris.dt.predictions <- predict(iris.dt.model, iris.test, type = "class")
## Extract the test data species to build the confusion matrix
iris.dt.confusion <- table(iris.dt.predictions, iris.test$Species)
print(iris.dt.confusion)
## calculate accuracy, precision, recall, F1
iris.dt.accuracy <- sum(diag(iris.dt.confusion)) / sum(iris.dt.confusion)
print(iris.dt.accuracy)
iris.dt.precision <- iris.dt.confusion[2,2] / sum(iris.dt.confusion[2,])
print(iris.dt.precision)
iris.dt.recall <- iris.dt.confusion[2,2] / sum(iris.dt.confusion[,2])
print(iris.dt.recall)
iris.dt.F1 <- 2 * iris.dt.precision * iris.dt.recall / (iris.dt.precision + iris.dt.recall)
print(iris.dt.F1)
## EXERCISE
install.packages('installr')
library(installr)
updateR()
update.packages(checkBuilt = T, ask=F)
library(rpart)
## DATA EXPLORATION
## load the iris data in R
data(iris)
## explore the data set
str(iris)
dim(iris)
summary(iris)
## BUILD MODEL
## randomly choose 70% of the data set as training data
set.seed(27)
iris.train.indices <- sample(1:nrow(iris), 0.7*nrow(iris), replace=F)
iris.train <- iris[iris.train.indices,]
dim(iris.train)
## select the 30% left as the testing data
iris.test <- iris[-iris.train.indices,]
dim(iris.test)
## You could also do this
#iris.test.indices <- setdiff(1:nrow(iris),random.rows.train)
#iris.test <- iris[random.rows.test,]
## Setting control parameters for rpart
## Check ?rpart.control for what the parameters do
iris.dt.parameters <- rpart.control(minsplit=20, minbucket=7, cp=0.01, maxdepth=30)
## Fit decision model to training set
## Use parameters from above and Gini index for splitting
iris.dt.model <- rpart(Species ~ ., data = iris.train,
control=iris.dt.parameters, parms=list(split="gini"))
## VISUALIZE THE MODEL
## plot the tree structure
plot(iris.dt.model)
title(main = "Decision Tree Model of Iris Data")
text(iris.dt.model, use.n = TRUE)
## print the tree structure
summary(iris.dt.model)
## MODEL EVALUATION
## make prediction using decision model
iris.dt.predictions <- predict(iris.dt.model, iris.test, type = "class")
## Extract the test data species to build the confusion matrix
iris.dt.confusion <- table(iris.dt.predictions, iris.test$Species)
print(iris.dt.confusion)
## calculate accuracy, precision, recall, F1
iris.dt.accuracy <- sum(diag(iris.dt.confusion)) / sum(iris.dt.confusion)
print(iris.dt.accuracy)
iris.dt.precision <- iris.dt.confusion[2,2] / sum(iris.dt.confusion[2,])
print(iris.dt.precision)
iris.dt.recall <- iris.dt.confusion[2,2] / sum(iris.dt.confusion[,2])
print(iris.dt.recall)
iris.dt.F1 <- 2 * iris.dt.precision * iris.dt.recall / (iris.dt.precision + iris.dt.recall)
print(iris.dt.F1)
## EXERCISE
## Another library called "party" can be also used to build decision trees.
??binom
rbinom
?rbinom
rbinom(2,20,.08)
pbinom(2,20,.08)
dbinom(2,20,.08)
dbinom(3:20,20,.08)
sum(dbinom(3:20,20,.08))
update.packages()
y
library(ggpairs)
library(GGally)
ggpairs(iris, ggplot2::aes(color="Species"))
ggpairs(iris, color="Species")
print(ggpairs)
library(GGally)
ggpairs(iris, ggplot2::aes(color="Species"))
?ggparis
?ggpairs
ggpairs(iris, ggplot2::aes(color=Species))
data(mtcars)
colnames(mtcars)
names(mtcars)
?names
rownames(mtcars)
names(mtcars)
range(mtcars[[c('disp','hp')]])
mtcars.4 <- mtcars[mtcars$cyl==4]
dbinom(20,.08,2)
?dbinom
dbinom(20, 2, .08)
dbinom(2, 20./08)
dbinom(2, 20,.08)
dbinom(20, 20,.08)
dbinom(1, 20,.08)
dbinom(0, 20,.08)
dbinom(0, 20,.92)
dbinom(2, 20,.92)
dbinom(0:10, 20,.92)
dbinom(2, 20,.9)
pbinom(2, 20,.9)
pbinom(2:5, 20,.9)
pbinom(2, 20,.8)
pbinom(1, 20,.8)
pbinom(20, 20,.8)
pbinom(2, 20,.2)
sum(dbinom(0:20,20,.08))
sum(dbinom(0:2,20,.08))
data()
data(Titanic)
summary(Titanic)
summary(cars)
summary(co2)
summary(uspop)
summary(BJsales)
summary(mtcars)
mtcars$am <- as.factor(mtcars$am)
names(mtcars$am)
names(mtcars$am) <- c('automatic', 'manual')
names(mtcars$am)
!(names(mtcars$am) %in% c('drat','gear'))
!(names(mtcars$am) %in% c('drat','gear'))
(names(mtcars$am) %in% c('drat','gear'))
!(names(mtcars$am) %in% c('drat','gear'))
names(mtcars$am) %in% c('drat','gear')
which(names(mtcars$am) %in% c('drat','gear'))
names(mtcars) %in% c('drat','gear')
names(mtcars)
1-(1/6)^2-(5/6)^2
1-(2/6)^2-(4/6)^2
1-(5/7)^2-(2/7)^2
1-(1/5)^2-(4/5)^2
7/12*.408+5/12*.32
?log
0*log2(0)
log2(0)
-(1/6)log2(1/6)-(5/6)log2(5/6)
-(1/6)*log2(1/6)-(5/6)*log2(5/6)
-(2/6)*log2(2/6)-(4/6)*log2(4/6)
25/49
(5/7)^2
-(1/2)*log2(1/2)-(1/2)*log2(1/2)
?dbinom
dbinom(20,2,.08)
dbinom(2,20,.08)
x = seq(0,1,.01)
c <- ln(x)
c <- log(x)
?log
x = seq(0.01,1,.01)
c <- log(x)
library(ggplot)
library(ggplot2)
g <- ggplot(cbi
df <- as.data.frame(cbind(x,c))
df
head(df)
g <- ggplot(df, aes(x=x, y=c))
g + geom_line()
c <- -log(x)
df <- as.data.frame(cbind(x,c))
g + geom_line()
g <- ggplot(df, aes(x=x, y=c))
g + geom_line()
df$d <- 1-df$c
head(df)
df$d <- -log(1-x)
head(df)
g + geom_line() + theme(axis.title=element_text(size=20))
g + geom_line() + theme(axis.title=element_text(size=30))
g + geom_line() + theme(axis.title=element_text(size=30), axis.text=element_text(size=15))
g + geom_line() + theme(axis.title=element_text(size=30), axis.text=element_text(size=15), line=element_line(size=3))
g + geom_line(size=3) + theme(axis.title=element_text(size=30), axis.text=element_text(size=15))
g + geom_line(size=3) + theme(axis.title=element_text(size=30), axis.text=element_text(size=20))
g + geom_line(size=3) + theme(axis.title=element_text(size=30), axis.text=element_text(size=20)) + labs(x="h\u03f4(x)")
g + geom_line(size=3) + theme(axis.title=element_text(size=30), axis.text=element_text(size=20)) + labs(x="h_\u03f4(x)")
g + geom_line(size=3) + theme(axis.title=element_text(size=30), axis.text=element_text(size=20)) + labs(x=expression(h[\u03f4](x))
g + geom_line(size=3) + theme(axis.title=element_text(size=30), axis.text=element_text(size=20)) + labs(x=expression(h["\u03f4"](x))
)
g + geom_line(size=3) + theme(axis.title=element_text(size=30), axis.text=element_text(size=20)) + labs(x=expression(h["\theta"](x)))
g + geom_line(size=3) + theme(axis.title=element_text(size=30), axis.text=element_text(size=20)) + labs(x=expression(h["\u03b8"](x)))
g + geom_line(size=3) + theme(axis.title=element_text(size=30), axis.text=element_text(size=20)) + labs(x=expression(h["theta"](x)))
g + geom_line(size=3) + theme(axis.title=element_text(size=30), axis.text=element_text(size=20)) + labs(x=expression(h[theta](x)))
g + geom_line(size=3) + theme(axis.title=element_text(size=30), axis.text=element_text(size=20)) + labs(x=expression(h[theta](x)), y=expression(ln(P[theta](x))))
g + geom_line(size=3) + theme(axis.title=element_text(size=30), axis.text=element_text(size=20)) + labs(x=expression(h[theta](x)), y=expression(ln(P[theta](x))), title="Cost for y=0")
g + geom_line(size=3) + theme(axis.title=element_text(size=30), axis.text=element_text(size=20), title=element_text(size=30) + labs(x=expression(h[theta](x)), y=expression(ln(P[theta](x))), title="Cost for y=0")
g + geom_line(size=3) + theme(axis.title=element_text(size=30), axis.text=element_text(size=20), title=element_text(size=30)) + labs(x=expression(h[theta](x)), y=expression(ln(P[theta](x))), title="Cost for y=0")
g + geom_line(size=3) + theme(axis.title=element_text(size=30), axis.text=element_text(size=20), title=element_text(size=30)) + labs(x=expression(h[theta](x)), y=expression(ln(P[theta](x))), title="Cost for y=1")
g + geom_line(aes(y=d),size=3) + theme(axis.title=element_text(size=30), axis.text=element_text(size=20), title=element_text(size=30)) + labs(x=expression(h[theta](x)), y=expression(ln(P[theta](x))), title="Cost for y=0")
g <- ggplot(df, aes(x=x))
g + geom_line(aes(y=d),size=3) + theme(axis.title=element_text(size=30), axis.text=element_text(size=20), title=element_text(size=30)) + labs(x=expression(h[theta](x)), y=expression(ln(P[theta](x))), title="Cost for y=0")
a <- c(.84,.86,.83,.85,.79,.84,.86,.85,.89,.83)
mean(a)
var(a)
stdev(a)
sqrt(var(a))
b <- c(.79,.78,.81,.79,.85,.76,.82,.71,.75,.76)
mean(b)
sqrt(var(b))
d <- c(.75,.83,.76,.83,.65,.80,.74,.76,.77,.79)
mean(d)
sqrt(var(d))
?sample
sample(1:10, 7)
sample(1:10, 7, replace=T)
sample(1:10, 7, replace=T)
sqrt(40)
sqrt(8)
library(xyplot)
library(lattice)
?xyplot
library(lattice)
data = irirs
data = iris
xyplot(Sepal.Length ~ Species, data, groups="Sepal.Width")
xyplot(Sepal.Length ~ Sepal.Width, data, groups="Species")
xyplot(Sepal.Length ~ Sepal.Width, data, groups=Species)
library(lsa)
?cosine
a <- c(5,4,1,4)
b <- c(3,1,1,1)
c <- c(4,2,5,2)
?matrix
g <- c(5,1,2,1)
t <- c(4,3,2,5)
l <- c(4,2,5,1)
d <- c(1,5,2,2)
s <- c(5,3,5,2)
cosine(a,g)
?cosine
cosine(a,t)
cosine(a,l)
cosine(a,d)
cosine(a,s)
cosine(b,g)
cosine(b,t)
cosine(b,l)
cosine(b,d)
cosine(b,s)
cosine(c,g)
cosine(c,t)
cosine(c,l)
cosine(c,d)
cosine(c,s)
densityplot(~ Petal.Width, data=iris, groups=Species,
xlab=list(label="Kernel Density of Petal Width", fontsize=20), ylab="",
main=list(label="Density of Petal Width by Species", fontsize=24),
auto.key=list(corner=c(0,0), x=0.4, y=0.8, cex=2), scales=list(cex=1.5)
)
library
library(lattice)
densityplot(~ Petal.Width, data=iris, groups=Species,
xlab=list(label="Kernel Density of Petal Width", fontsize=20), ylab="",
main=list(label="Density of Petal Width by Species", fontsize=24),
auto.key=list(corner=c(0,0), x=0.4, y=0.8, cex=2), scales=list(cex=1.5)
)
1-1/e
?ln
?log
1-exp(-1)
data(mtcars)
mtcars.4 <- mtcars[mtcars$cyl=4,]
mtcars.4 <- mtcars[mtcars$cyl==4,]
write.table(mtcars.4, "cars4cyl.csv", delimiter=',')
?write.table
write.csv(mtcars.4, "cars4cyl.csv")
write.table(mtcars.4, "cars4cyl.csv")
write.table(mtcars.4, "cars4cyl.csv", sep=',')
library(rpart)
library(bst)
?bst
library(xgboost)
install.packageS(
install.packages('xgboost')
library(xgboost_
library(xgboost)
?xgboost
library(randomForest)
setwd("C:/git/KaggleComps/BNP Claims Management")
# Pass in a train or test set
library(plyr)
library(caret)
library(randomForest)
bnp.process <- function(data) {
ID <- data$ID;
if(!is.null(data$target)) {
target <- data$target;
}
pca.list <- list(g8 = ~ v8 + v25 + v46 + v63 + v105,
g15 = ~ v15 + v32 + v73 + v86,
g17 = ~ v17 + v48 + v64 + v76,
g26 = ~ v26 + v43 + v60 + v116,
g29 = ~ v29 + v41 + v61 + v67 + v77 + v96,
g33 = ~ v33 + v55 + v83 + v111 + v121,
g34 = ~ v34 + v40 + v114,
g128 = ~ v108 + v109
);
drop.list <- append(c('v107', 'v22', 'v110', 'v125', 'v56'),
unlist(llply(pca.list, function(f) attr(terms(f),'term.labels')),
use.names=F)
);
data.drop <- data[,!(colnames(data) %in% drop.list)];
data.pca <- data.drop;
pca.cols <- llply(pca.list, function(l) {
name <- paste('g', attr(terms(l),'term.labels')[1], sep='');
pca.model <- preProcess(model.frame(l, data, na.action=NULL), c('center', 'scale', 'pca'));
pca.res <- predict(pca.model, model.frame(l, data, na.action=NULL));
colnames(pca.res) <- lapply(colnames(pca.res),paste,name,sep='_');
return(cbind(ID, pca.res));
});
l_ply(pca.cols, function(l) { data.pca <<- join(data.pca, l, by='ID')});
data.cln <- as.data.frame(llply(data.pca, function(l) {
if (is.factor(l)) {
if (any(is.na(l))) {
levels(l) <- append(levels(l),'');
l[is.na(l)] <- "";
}
return(l);
}
l.mean <- mean(l, na.rm=T);
l.std <- sd(l, na.rm=T);
l[is.na(l)] <- rnorm(l[is.na(l)], mean=l.mean, sd=l.std);
return(l);
}));
if (is.null(data$target)) {
data.cln$v71[data.cln$v71 %in% c('E', 'J')] <- 'F'
data.cln$v71 <- factor(data.cln$v71)
data.cln$v113[data.cln$v113 == 'K'] <- ""
data.cln$v113 <- factor(data.cln$v113)
}
return(data.cln);
}
kaggle.logloss <- function(actual, predicted) {
predicted <- as.numeric(lapply(predicted, function(p) max(min(p,1-10^(-15)),10^(-15))));
-(1/length(actual))*sum(actual*log(predicted)+(1-actual)*log(1-predicted));
}
set.seed(27)
train <- read.csv('train.csv')
test <- read.csv('test.csv')
train.cln <- bnp.process(train)
holdout.ind <- sample(1:nrow(train.cln), 0.1*nrow(train.cln),replace=F)
holdout <- train.cln[holdout.ind,]
train.cln.use <- train.cln[-holdout.ind,]
train.cln.use$target <- as.factor(train.cln.use)
